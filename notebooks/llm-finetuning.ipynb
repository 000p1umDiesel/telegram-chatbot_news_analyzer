{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# –î–æ–æ–±—É—á–µ–Ω–∏–µ Saiga LLaMA3 –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ö–µ—à—Ç–µ–≥–æ–≤ (–û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)\n",
    "\n",
    "**–ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è:** –í Kaggle  \n",
    "**–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:** Dataset —Å —Ñ–∞–π–ª–∞–º–∏ –∏–∑ prepare_finetune_data.ipynb  \n",
    "**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –î–æ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è  \n",
    "\n",
    "## üìã –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:\n",
    "1. GPU Runtime –≤ Kaggle (P100/T4/V100)\n",
    "2. Dataset —Å train_data.jsonl, test_data.jsonl, config.json\n",
    "3. –ò–Ω—Ç–µ—Ä–Ω–µ—Ç –≤–∫–ª—é—á–µ–Ω –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T09:23:24.640569Z",
     "iopub.status.busy": "2025-07-13T09:23:24.640296Z",
     "iopub.status.idle": "2025-07-13T09:23:28.143696Z",
     "shell.execute_reply": "2025-07-13T09:23:28.142998Z",
     "shell.execute_reply.started": "2025-07-13T09:23:24.640549Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "wandb_api = user_secrets.get_secret(\"wandb_api\")\n",
    "\n",
    "#wandb.login(key=wandb_api)\n",
    "! wandb login $wandb_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T09:23:28.145451Z",
     "iopub.status.busy": "2025-07-13T09:23:28.145164Z",
     "iopub.status.idle": "2025-07-13T09:25:39.995483Z",
     "shell.execute_reply": "2025-07-13T09:25:39.994665Z",
     "shell.execute_reply.started": "2025-07-13T09:23:28.145419Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫ (–∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –≤–µ—Ä—Å–∏–∏ –Ω–∞ –¥–µ–∫–∞–±—Ä—å 2024)\n",
    "!pip install -q transformers>=4.47.0\n",
    "!pip install -q peft>=0.13.0\n",
    "!pip install -q trl>=0.12.0\n",
    "!pip install -q bitsandbytes>=0.44.0\n",
    "!pip install -q accelerate>=1.1.0\n",
    "!pip install -q datasets>=3.1.0\n",
    "!pip install -q liger-kernel>=0.4.0  # –î–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T09:25:39.997391Z",
     "iopub.status.busy": "2025-07-13T09:25:39.997181Z",
     "iopub.status.idle": "2025-07-13T09:26:08.040368Z",
     "shell.execute_reply": "2025-07-13T09:26:08.039724Z",
     "shell.execute_reply.started": "2025-07-13T09:25:39.997369Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# –ò–º–ø–æ—Ä—Ç—ã\n",
    "from accelerate import notebook_launcher\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import Dataset\n",
    "import shutil\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "print(f\" CUDA –¥–æ—Å—Ç—É–ø–µ–Ω: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T09:26:08.041218Z",
     "iopub.status.busy": "2025-07-13T09:26:08.041031Z",
     "iopub.status.idle": "2025-07-13T09:26:08.050307Z",
     "shell.execute_reply": "2025-07-13T09:26:08.049735Z",
     "shell.execute_reply.started": "2025-07-13T09:26:08.041201Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏\n",
    "# –ó–∞–º–µ–Ω–∏—Ç–µ '/kaggle/input/your-dataset' –Ω–∞ –ø—É—Ç—å –∫ –≤–∞—à–µ–º—É dataset –≤ Kaggle\n",
    "DATASET_PATH = \"/kaggle/input/dataset-for-llm\"  # –ò–∑–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à dataset\n",
    "\n",
    "config_file = Path(DATASET_PATH) / \"config.json\"\n",
    "with open(config_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(\" –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞:\")\n",
    "print(f\"   –ú–æ–¥–µ–ª—å: {config['model_config']['base_model']}\")\n",
    "print(f\"   –û–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {config['dataset_info']['train_samples']}\")\n",
    "print(f\"   –¢–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {config['dataset_info']['test_samples']}\")\n",
    "print(f\"   LoRA rank: {config['training_config']['lora_r']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T09:26:08.051881Z",
     "iopub.status.busy": "2025-07-13T09:26:08.051642Z",
     "iopub.status.idle": "2025-07-13T09:26:08.140299Z",
     "shell.execute_reply": "2025-07-13T09:26:08.139674Z",
     "shell.execute_reply.started": "2025-07-13T09:26:08.051856Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ JSONL —Ñ–∞–π–ª–∞\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—É—á–∞—é—â–∏–µ –∏ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "train_file = Path(DATASET_PATH) / \"train_data_clear.jsonl\"\n",
    "test_file = Path(DATASET_PATH) / \"test_data_clear.jsonl\"\n",
    "\n",
    "train_data = load_jsonl(train_file)\n",
    "test_data = load_jsonl(test_file)\n",
    "\n",
    "print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ:\")\n",
    "print(f\"   –û–±—É—á–∞—é—â–∏—Ö: {len(train_data)}\")\n",
    "print(f\"   –¢–µ—Å—Ç–æ–≤—ã—Ö: {len(test_data)}\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "print(f\"\\n –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –¥–∞–Ω–Ω—ã—Ö:\")\n",
    "sample = train_data[0]\n",
    "print(f\"   –ö–ª—é—á–∏ –≤ –¥–∞–Ω–Ω—ã—Ö: {list(sample.keys())}\")\n",
    "print(f\"   –ü—Ä–∏–º–µ—Ä input: {sample['input'][:100]}...\")\n",
    "print(f\"   –ü—Ä–∏–º–µ—Ä output: {sample['output']}\")\n",
    "print(f\"   –î–ª–∏–Ω–∞ –ø–æ–ª—è 'text': {len(sample['text'])} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ–ª–µ 'text' —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "if (\n",
    "    \"<s>system\" in sample[\"text\"]\n",
    "    and \"<s>user\" in sample[\"text\"]\n",
    "    and \"<s>assistant\" in sample[\"text\"]\n",
    "):\n",
    "    print(\"–î–∞–Ω–Ω—ã–µ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ ChatML –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\")\n",
    "else:\n",
    "    print(\"–î–∞–Ω–Ω—ã–µ –ù–ï –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ ChatML!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T09:26:08.141224Z",
     "iopub.status.busy": "2025-07-13T09:26:08.141016Z",
     "iopub.status.idle": "2025-07-13T09:26:08.146720Z",
     "shell.execute_reply": "2025-07-13T09:26:08.146074Z",
     "shell.execute_reply.started": "2025-07-13T09:26:08.141207Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–æ–∫ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "output_dir = \"./hashtag_lora_model\"\n",
    "merged_dir = \"./merged_hashtag_model\"\n",
    "final_dir = \"./final_model_for_ollama\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(merged_dir, exist_ok=True)\n",
    "os.makedirs(final_dir, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ –ü–∞–ø–∫–∏ —Å–æ–∑–¥–∞–Ω—ã –≤ Kaggle:\")\n",
    "print(f\"   LoRA –º–æ–¥–µ–ª—å: {output_dir}\")\n",
    "print(f\"   –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å: {merged_dir}\")\n",
    "print(f\"   –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å: {final_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T09:26:08.147728Z",
     "iopub.status.busy": "2025-07-13T09:26:08.147507Z",
     "iopub.status.idle": "2025-07-13T09:26:08.165126Z",
     "shell.execute_reply": "2025-07-13T09:26:08.164546Z",
     "shell.execute_reply.started": "2025-07-13T09:26:08.147704Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#  –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: –ü—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ö–µ—à—Ç–µ–≥–æ–≤\n",
    "\n",
    "print(\"–î–µ—Ç–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ö–µ—à—Ç–µ–≥–æ–≤:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤\n",
    "for i, sample in enumerate(train_data[:3]):\n",
    "    print(f\"\\nüìã –ü—Ä–∏–º–µ—Ä {i+1}:\")\n",
    "    print(f\"   Input (–Ω–æ–≤–æ—Å—Ç—å): {sample['input'][:80]}...\")\n",
    "    print(f\"   Output (—Ö–µ—à—Ç–µ–≥–∏): {sample['output']}\")\n",
    "\n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–ª–µ 'text' - —ç—Ç–æ —Ç–æ, –Ω–∞ —á–µ–º –±—É–¥–µ—Ç –æ–±—É—á–∞—Ç—å—Å—è –º–æ–¥–µ–ª—å\n",
    "    text_parts = sample[\"text\"].split(\"<s>\")\n",
    "    for part in text_parts:\n",
    "        if part.startswith(\"system\"):\n",
    "            system_msg = part.replace(\"system\\n\", \"\").replace(\"</s>\", \"\")\n",
    "            print(f\"   System: {system_msg[:50]}...\")\n",
    "        elif part.startswith(\"user\"):\n",
    "            user_msg = part.replace(\"user\\n\", \"\").replace(\"</s>\", \"\")\n",
    "            print(f\"   User: {user_msg[:50]}...\")\n",
    "        elif part.startswith(\"assistant\"):\n",
    "            assistant_msg = part.replace(\"assistant\\n\", \"\").replace(\"</s>\", \"\")\n",
    "            print(f\"   Assistant: {assistant_msg}\")\n",
    "\n",
    "            # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: –°–æ–≤–ø–∞–¥–∞—é—Ç –ª–∏ —Ö–µ—à—Ç–µ–≥–∏?\n",
    "            if assistant_msg.strip() == sample[\"output\"].strip():\n",
    "                print(f\"   –•–µ—à—Ç–µ–≥–∏ —Å–æ–≤–ø–∞–¥–∞—é—Ç - –æ–±—É—á–µ–Ω–∏–µ –±—É–¥–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º!\")\n",
    "            else:\n",
    "                print(f\"   –û–®–ò–ë–ö–ê: –•–µ—à—Ç–µ–≥–∏ –ù–ï —Å–æ–≤–ø–∞–¥–∞—é—Ç!\")\n",
    "                print(f\"      –í 'text': '{assistant_msg.strip()}'\")\n",
    "                print(f\"      –í 'output': '{sample['output'].strip()}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"–í–´–í–û–î: –ú–æ–¥–µ–ª—å –±—É–¥–µ—Ç –æ–±—É—á–∞—Ç—å—Å—è –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ö–µ—à—Ç–µ–≥–∏ –∏–∑ –ø–æ–ª—è 'output'\")\n",
    "print(\"–§–æ—Ä–º–∞—Ç –æ–±—É—á–µ–Ω–∏—è: ChatML —Å —Å–∏—Å—Ç–µ–º–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º + –Ω–æ–≤–æ—Å—Ç—å + —Ö–µ—à—Ç–µ–≥–∏\")\n",
    "print(\"–≠—Ç–æ –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ö–µ—à—Ç–µ–≥–æ–≤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T09:26:08.165961Z",
     "iopub.status.busy": "2025-07-13T09:26:08.165737Z",
     "iopub.status.idle": "2025-07-13T09:26:08.183088Z",
     "shell.execute_reply": "2025-07-13T09:26:08.182322Z",
     "shell.execute_reply.started": "2025-07-13T09:26:08.165944Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "\n",
    "\n",
    "logging.enable_progress_bar()\n",
    "logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T09:26:08.184161Z",
     "iopub.status.busy": "2025-07-13T09:26:08.183933Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "\n",
    "def traning_this():\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"llm-hashtag-generation\",\n",
    "        name=f\"lora-r{config['training_config']['lora_r']}-lr{config['training_config']['learning_rate']}\",\n",
    "        config=config[\"training_config\"],\n",
    "    )\n",
    "\n",
    "    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
    "    model_name = config[\"model_config\"][\"base_model\"]\n",
    "    print(f\"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_name}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        r=config[\"training_config\"][\"lora_r\"],\n",
    "        lora_alpha=config[\"training_config\"][\"lora_alpha\"],\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    print(\"–ú–æ–¥–µ–ª—å –∏ LoRA –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã\")\n",
    "\n",
    "    train_texts = [item[\"text\"] for item in train_data]\n",
    "    test_texts = [item[\"text\"] for item in test_data]\n",
    "\n",
    "    train_dataset = Dataset.from_dict({\"text\": train_texts})\n",
    "    test_dataset = Dataset.from_dict({\"text\": test_texts})\n",
    "\n",
    "    training_args = SFTConfig(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=config[\"training_config\"][\"epochs\"],\n",
    "        per_device_train_batch_size=config[\"training_config\"][\"batch_size\"],\n",
    "        per_device_eval_batch_size=config[\"training_config\"][\"batch_size\"],\n",
    "        gradient_accumulation_steps=4,  # –î–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        save_steps=100,\n",
    "        logging_steps=10,\n",
    "        learning_rate=config[\"training_config\"][\"learning_rate\"],\n",
    "        weight_decay=0.001,\n",
    "        fp16=True,\n",
    "        bf16=False,\n",
    "        max_grad_norm=0.3,\n",
    "        max_steps=-1,\n",
    "        warmup_ratio=0.03,\n",
    "        group_by_length=True,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        report_to=\"wandb\",\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=50,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        run_name=wandb.run.name,\n",
    "        # SFT-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
    "        max_seq_length=512,\n",
    "        dataset_text_field=\"text\",\n",
    "        packing=False,\n",
    "        use_liger_kernel=False,\n",
    "        remove_unused_columns=False,\n",
    "        # dataloader_num_workers = 0\n",
    "    )\n",
    "\n",
    "    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è TRL 0.12.0\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        peft_config=lora_config,\n",
    "        args=training_args,\n",
    "        processing_class=tokenizer,\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ –¢—Ä–µ–Ω–µ—Ä –Ω–∞—Å—Ç—Ä–æ–µ–Ω —Å –∞–∫—Ç—É–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\")\n",
    "\n",
    "    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "    trainer.train()\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    # –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (LoRA + –±–∞–∑–æ–≤–∞—è)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, device_map=\"cpu\", trust_remote_code=True\n",
    "    )\n",
    "    merged_model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "    merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
    "    merged_model.save_pretrained(os.path.join(output_dir, \"merged_model\"))\n",
    "    tokenizer.save_pretrained(os.path.join(output_dir, \"merged_model\"))\n",
    "\n",
    "    return {\n",
    "        \"trainer\": trainer,\n",
    "        \"model\": merged_model,\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"output_dir\": output_dir,\n",
    "        \"merged_model_path\": os.path.join(output_dir, \"merged_model\"),\n",
    "    }\n",
    "\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è\n",
    "results = notebook_launcher(traning_this, num_processes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = results[\"trainer\"]\n",
    "model = results[\"model\"]\n",
    "output_dir = results[\"output_dir\"]\n",
    "merged_model_path = results[\"merged_model_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è Ollama\n",
    "print(\"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è Ollama...\")\n",
    "\n",
    "# –ö–æ–ø–∏—Ä—É–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –≤ —Ñ–∏–Ω–∞–ª—å–Ω—É—é –ø–∞–ø–∫—É\n",
    "shutil.copytree(merged_dir, final_dir, dirs_exist_ok=True)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º Modelfile –¥–ª—è Ollama\n",
    "modelfile_content = '''FROM ./\n",
    "\n",
    "SYSTEM \"\"\"–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ö–µ—à—Ç–µ–≥–æ–≤ –¥–ª—è –Ω–æ–≤–æ—Å—Ç–µ–π, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–π –Ω–∞ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.\n",
    "\n",
    "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ö–µ—à—Ç–µ–≥–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.\n",
    "\n",
    "–ü—Ä–∞–≤–∏–ª–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:\n",
    "1. –°–æ–∑–¥–∞–≤–∞–π 3-5 —Ö–µ—à—Ç–µ–≥–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π –Ω–æ–≤–æ—Å—Ç–∏\n",
    "2. –•–µ—à—Ç–µ–≥–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –∏ –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω—ã–º–∏\n",
    "3. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫\n",
    "4. –ù–µ –¥–æ–±–∞–≤–ª—è–π —Å–∏–º–≤–æ–ª # –≤ –æ—Ç–≤–µ—Ç–µ\n",
    "5. –†–∞–∑–¥–µ–ª—è–π —Ö–µ—à—Ç–µ–≥–∏ –∑–∞–ø—è—Ç—ã–º–∏\n",
    "6. –§–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ–º–∞—Ö –∏ —Å–æ–±—ã—Ç–∏—è—Ö\n",
    "\n",
    "–û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ —Ö–µ—à—Ç–µ–≥–∞–º–∏, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤.\"\"\"\n",
    "\n",
    "PARAMETER temperature 0.1\n",
    "PARAMETER top_p 0.9\n",
    "PARAMETER repeat_penalty 1.1\n",
    "PARAMETER num_ctx 2048\n",
    "PARAMETER stop \"<s>\"\n",
    "PARAMETER stop \"</s>\"\n",
    "'''\n",
    "\n",
    "with open(f\"{final_dir}/Modelfile\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(modelfile_content)\n",
    "\n",
    "print(f\"‚úÖ Modelfile —Å–æ–∑–¥–∞–Ω –≤ {final_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞–Ω–∏–µ README —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏\n",
    "instructions = f\"\"\"# üöÄ –î–æ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å Saiga LLaMA3 –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ö–µ—à—Ç–µ–≥–æ–≤\n",
    "\n",
    "## üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ–±—É—á–µ–Ω–∏–∏\n",
    "- –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: {model_name}\n",
    "- –û–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {config['dataset_info']['train_samples']}\n",
    "- –≠–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è: {config['training_config']['epochs']}\n",
    "- LoRA rank: {config['training_config']['lora_r']}\n",
    "- –í–µ—Ä—Å–∏—è TRL: 0.12.0+ (–¥–µ–∫–∞–±—Ä—å 2024)\n",
    "\n",
    "## üîß –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤ Ollama\n",
    "\n",
    "1. –°–∫–∞—á–∞–π—Ç–µ –∏ —Ä–∞—Å–ø–∞–∫—É–π—Ç–µ —ç—Ç—É –ø–∞–ø–∫—É\n",
    "2. –ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤ –ø–∞–ø–∫—É —Å –º–æ–¥–µ–ª—å—é:\n",
    "   ```bash\n",
    "   cd path/to/final_model_for_ollama\n",
    "   ```\n",
    "\n",
    "3. –°–æ–∑–¥–∞–π—Ç–µ –º–æ–¥–µ–ª—å –≤ Ollama:\n",
    "   ```bash\n",
    "   ollama create saiga-hashtag-pro -f Modelfile\n",
    "   ```\n",
    "\n",
    "4. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∫—É:\n",
    "   ```bash\n",
    "   ollama list\n",
    "   ```\n",
    "\n",
    "## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "\n",
    "```bash\n",
    "ollama run saiga-hashtag-pro \"–¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –±–∞–Ω–∫ –ø–æ–≤—ã—Å–∏–ª –∫–ª—é—á–µ–≤—É—é —Å—Ç–∞–≤–∫—É\"\n",
    "```\n",
    "\n",
    "## –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "\n",
    "**–í—Ö–æ–¥:** \"–ù–æ–≤—ã–π iPhone 15 –ø–æ—Å—Ç—É–ø–∏–ª –≤ –ø—Ä–æ–¥–∞–∂—É –≤ –†–æ—Å—Å–∏–∏\"\n",
    "**–í—ã—Ö–æ–¥:** \"—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, —Å–º–∞—Ä—Ç—Ñ–æ–Ω—ã, –ø—Ä–æ–¥–∞–∂–∏\"\n",
    "\n",
    "**–í—Ö–æ–¥:** \"–°–±–æ—Ä–Ω–∞—è –†–æ—Å—Å–∏–∏ –≤—ã–∏–≥—Ä–∞–ª–∞ –º–∞—Ç—á –ø–æ —Ñ—É—Ç–±–æ–ª—É\"\n",
    "**–í—ã—Ö–æ–¥:** \"—Å–ø–æ—Ä—Ç, —Ñ—É—Ç–±–æ–ª, —Å–±–æ—Ä–Ω–∞—è_—Ä–æ—Å—Å–∏–∏\"\n",
    "\n",
    "## –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —ç—Ç–æ–π –≤–µ—Ä—Å–∏–∏\n",
    "- –û–±—É—á–µ–Ω–∞ –Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç—è—Ö\n",
    "- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫ (TRL 0.12.0+)\n",
    "- –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏\n",
    "- –ü—Ä–∞–≤–∏–ª—å–Ω–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ö–µ—à—Ç–µ–≥–æ–≤\n",
    "- –ü—Ä–æ–≤–µ—Ä–µ–Ω–∞ –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "##  –§–æ—Ä–º–∞—Ç –æ–±—É—á–µ–Ω–∏—è\n",
    "- **–î–∞–Ω–Ω—ã–µ**: ChatML —Ñ–æ—Ä–º–∞—Ç —Å —Å–∏—Å—Ç–µ–º–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º\n",
    "- **–ü–æ–ª–µ –æ–±—É—á–µ–Ω–∏—è**: `text` (–ø–æ–ª–Ω—ã–π –¥–∏–∞–ª–æ–≥)\n",
    "- **–¶–µ–ª–µ–≤—ã–µ —Ö–µ—à—Ç–µ–≥–∏**: –∏–∑ –ø–æ–ª—è `output`\n",
    "- **–ü—Ä–æ–≤–µ—Ä–∫–∞**: –•–µ—à—Ç–µ–≥–∏ –≤ `text` –∏ `output` —Å–æ–≤–ø–∞–¥–∞—é—Ç ‚úÖ\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{final_dir}/README.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(instructions)\n",
    "\n",
    "print(f\"‚úÖ README —Å–æ–∑–¥–∞–Ω –≤ {final_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7847595,
     "sourceId": 12449161,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
