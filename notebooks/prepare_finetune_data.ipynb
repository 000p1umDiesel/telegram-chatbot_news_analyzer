{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Kaggle\n",
        "\n",
        "–≠—Ç–æ—Ç notebook –∏–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ PostgreSQL –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –∏—Ö –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤ Kaggle.\n",
        "\n",
        "**–ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è:** –ù–∞ –≤–∞—à–µ–º –∫–æ–º–ø—å—é—Ç–µ—Ä–µ  \n",
        "**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –§–∞–π–ª—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤ Kaggle  \n",
        "**–°–ª–µ–¥—É—é—â–∏–π —à–∞–≥:** –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –≤ Kaggle –∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ –ò–º–ø–æ—Ä—Ç—ã –≥–æ—Ç–æ–≤—ã\n"
          ]
        }
      ],
      "source": [
        "# –ò–º–ø–æ—Ä—Ç—ã\n",
        "import asyncio\n",
        "import asyncpg\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"üì¶ –ò–º–ø–æ—Ä—Ç—ã –≥–æ—Ç–æ–≤—ã\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÅ –ü–∞–ø–∫–∞ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞: ./kaggle_dataset\n"
          ]
        }
      ],
      "source": [
        "# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î\n",
        "DB_CONFIG = {\n",
        "    \"host\": \"localhost\",\n",
        "    \"port\": 5432,\n",
        "    \"database\": \"news_analyzer\",\n",
        "    \"user\": \"staspalatov\",\n",
        "    \"password\": \"2004—Å—Ç–∞—Å\",\n",
        "}\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —ç–∫—Å–ø–æ—Ä—Ç–∞ –¥–ª—è Kaggle\n",
        "OUTPUT_DIR = \"./kaggle_dataset\"\n",
        "MAX_SAMPLES = 10000\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"üìÅ –ü–∞–ø–∫–∞ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîå –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL...\n",
            "üìä –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ (–º–∞–∫—Å. 10000)...\n",
            "‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ 127 –∑–∞–ø–∏—Å–µ–π\n"
          ]
        }
      ],
      "source": [
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ PostgreSQL\n",
        "async def load_training_data():\n",
        "    \"\"\"–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏–∑ PostgreSQL\"\"\"\n",
        "\n",
        "    try:\n",
        "        print(\"üîå –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL...\")\n",
        "        conn = await asyncpg.connect(**DB_CONFIG)\n",
        "\n",
        "        query = \"\"\"\n",
        "        SELECT \n",
        "            m.text,\n",
        "            a.hashtags,\n",
        "            a.sentiment,\n",
        "            m.channel_title\n",
        "        FROM messages m\n",
        "        JOIN analyses a ON m.message_id = a.message_id\n",
        "        WHERE \n",
        "            LENGTH(m.text) BETWEEN 50 AND 2000\n",
        "            AND a.hashtags IS NOT NULL\n",
        "            AND a.hashtags != '[]'\n",
        "            AND a.hashtags != 'null'\n",
        "            AND jsonb_array_length(a.hashtags::jsonb) BETWEEN 2 AND 10\n",
        "        ---    AND m.text NOT LIKE '%@%'\n",
        "        ---    AND m.text NOT LIKE '%http%'\n",
        "        ORDER BY m.date DESC\n",
        "        LIMIT $1\n",
        "        \"\"\"\n",
        "\n",
        "        print(f\"üìä –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ (–º–∞–∫—Å. {MAX_SAMPLES})...\")\n",
        "        rows = await conn.fetch(query, MAX_SAMPLES)\n",
        "        await conn.close()\n",
        "\n",
        "        print(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(rows)} –∑–∞–ø–∏—Å–µ–π\")\n",
        "        return rows\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}\")\n",
        "        print(\"üí° –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ PostgreSQL –∑–∞–ø—É—â–µ–Ω –∏ –¥–æ—Å—Ç—É–ø–µ–Ω\")\n",
        "        return []\n",
        "\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
        "raw_data = await load_training_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ 127 –ø—Ä–∏–º–µ—Ä–æ–≤\n"
          ]
        }
      ],
      "source": [
        "# –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö\n",
        "def prepare_training_examples(raw_data):\n",
        "    \"\"\"–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–∏–º–µ—Ä—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è Saiga LLaMA3\"\"\"\n",
        "\n",
        "    training_examples = []\n",
        "    system_prompt = \"–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ö–µ—à—Ç–µ–≥–æ–≤ –¥–ª—è –Ω–æ–≤–æ—Å—Ç–µ–π. –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π 3-5 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ö–µ—à—Ç–µ–≥–æ–≤ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.\"\n",
        "\n",
        "    for row in raw_data:\n",
        "        try:\n",
        "            hashtags = json.loads(row[\"hashtags\"])\n",
        "            hashtags_str = \", \".join(hashtags)\n",
        "\n",
        "            # –§–æ—Ä–º–∞—Ç ChatML –¥–ª—è Saiga LLaMA3\n",
        "            full_prompt = f\"<s>system\\n{system_prompt}</s>\\n<s>user\\n{row['text']}</s>\\n<s>assistant\\n{hashtags_str}</s>\"\n",
        "\n",
        "            training_examples.append(\n",
        "                {\n",
        "                    \"text\": full_prompt,\n",
        "                    \"input\": row[\"text\"],\n",
        "                    \"output\": hashtags_str,\n",
        "                    \"channel\": row[\"channel_title\"],\n",
        "                    \"sentiment\": row[\"sentiment\"],\n",
        "                }\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø–∏—Å–∏: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(training_examples)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "    return training_examples\n",
        "\n",
        "\n",
        "# –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã\n",
        "if raw_data:\n",
        "    training_examples = prepare_training_examples(raw_data)\n",
        "else:\n",
        "    print(\"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏\")\n",
        "    training_examples = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ: 101 train, 26 test\n",
            "\n",
            "üéâ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ ./kaggle_dataset/\n",
            "üìÑ –§–∞–π–ª—ã:\n",
            "   - train_data.jsonl (101 –∑–∞–ø–∏—Å–µ–π)\n",
            "   - test_data.jsonl (26 –∑–∞–ø–∏—Å–µ–π)\n",
            "   - config.json\n",
            "   - README.md\n"
          ]
        }
      ],
      "source": [
        "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Kaggle\n",
        "def save_for_kaggle(training_examples):\n",
        "    \"\"\"–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤ Kaggle\"\"\"\n",
        "\n",
        "    if not training_examples:\n",
        "        print(\"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\")\n",
        "        return\n",
        "\n",
        "    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test\n",
        "    split_idx = int(len(training_examples) * 0.8)\n",
        "    train_data = training_examples[:split_idx]\n",
        "    test_data = training_examples[split_idx:]\n",
        "\n",
        "    print(f\"üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ: {len(train_data)} train, {len(test_data)} test\")\n",
        "\n",
        "    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ JSONL —Ñ–∞–π–ª–æ–≤\n",
        "    train_file = Path(OUTPUT_DIR) / \"train_data.jsonl\"\n",
        "    test_file = Path(OUTPUT_DIR) / \"test_data.jsonl\"\n",
        "\n",
        "    with open(train_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for item in train_data:\n",
        "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    with open(test_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for item in test_data:\n",
        "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è Kaggle\n",
        "    config = {\n",
        "        \"dataset_info\": {\n",
        "            \"total_samples\": len(training_examples),\n",
        "            \"train_samples\": len(train_data),\n",
        "            \"test_samples\": len(test_data),\n",
        "            \"created_at\": datetime.now().isoformat(),\n",
        "        },\n",
        "        \"model_config\": {\n",
        "            \"base_model\": \"IlyaGusev/saiga_llama3_8b\",\n",
        "            \"task\": \"hashtag_generation\",\n",
        "            \"format\": \"ChatML\",\n",
        "        },\n",
        "        \"training_config\": {\n",
        "            \"epochs\": 3,\n",
        "            \"batch_size\": 4,\n",
        "            \"learning_rate\": 2e-4,\n",
        "            \"lora_r\": 16,\n",
        "            \"lora_alpha\": 32,\n",
        "        },\n",
        "    }\n",
        "\n",
        "    config_file = Path(OUTPUT_DIR) / \"config.json\"\n",
        "    with open(config_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # README –¥–ª—è Kaggle\n",
        "    readme_content = f\"\"\"# –î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è Saiga LLaMA3\n",
        "\n",
        "## üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è\n",
        "- –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(training_examples)}\n",
        "- –û–±—É—á–∞—é—â–∏—Ö: {len(train_data)}\n",
        "- –¢–µ—Å—Ç–æ–≤—ã—Ö: {len(test_data)}\n",
        "\n",
        "## üìÅ –§–∞–π–ª—ã\n",
        "- `train_data.jsonl` - –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ\n",
        "- `test_data.jsonl` - —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ  \n",
        "- `config.json` - –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è\n",
        "\n",
        "## üöÄ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ Kaggle\n",
        "–ó–∞–≥—Ä—É–∑–∏—Ç–µ —ç—Ç–∏ —Ñ–∞–π–ª—ã –∫–∞–∫ dataset –≤ Kaggle –∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ notebook –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.\n",
        "\"\"\"\n",
        "\n",
        "    readme_file = Path(OUTPUT_DIR) / \"README.md\"\n",
        "    with open(readme_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(readme_content)\n",
        "\n",
        "    print(f\"\\nüéâ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {OUTPUT_DIR}/\")\n",
        "    print(f\"üìÑ –§–∞–π–ª—ã:\")\n",
        "    print(f\"   - train_data.jsonl ({len(train_data)} –∑–∞–ø–∏—Å–µ–π)\")\n",
        "    print(f\"   - test_data.jsonl ({len(test_data)} –∑–∞–ø–∏—Å–µ–π)\")\n",
        "    print(f\"   - config.json\")\n",
        "    print(f\"   - README.md\")\n",
        "\n",
        "    return {\n",
        "        \"train_samples\": len(train_data),\n",
        "        \"test_samples\": len(test_data),\n",
        "        \"total_samples\": len(training_examples),\n",
        "    }\n",
        "\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ\n",
        "result = save_for_kaggle(training_examples)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ‚úÖ –ì–æ—Ç–æ–≤–æ!\n",
        "\n",
        "–î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ `./kaggle_dataset/`\n",
        "\n",
        "## üöÄ –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:\n",
        "\n",
        "1. **–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Å–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –≤ Kaggle** –∫–∞–∫ –Ω–æ–≤—ã–π dataset\n",
        "2. **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ notebook –¥–ª—è –æ–±—É—á–µ–Ω–∏—è** (kaggle_finetuning.ipynb)\n",
        "3. **–ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–π—Ç–µ –º–æ–¥–µ–ª—å –≤ Ollama** (—Å–ª–µ–¥—É–π—Ç–µ KAGGLE_TO_OLLAMA_GUIDE.md)\n",
        "\n",
        "## üìÅ –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:\n",
        "- `train_data.jsonl` - –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ\n",
        "- `test_data.jsonl` - —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
        "- `config.json` - –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è\n",
        "- `README.md` - –æ–ø–∏—Å–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
