{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üöÄ –î–æ–æ–±—É—á–µ–Ω–∏–µ Saiga LLaMA3 –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ö–µ—à—Ç–µ–≥–æ–≤\n",
        "\n",
        "**–ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è:** –í Kaggle  \n",
        "**–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:** Dataset —Å —Ñ–∞–π–ª–∞–º–∏ –∏–∑ prepare_finetune_data.ipynb  \n",
        "**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –î–æ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è  \n",
        "\n",
        "## üìã –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:\n",
        "1. GPU Runtime –≤ Kaggle\n",
        "2. Dataset —Å train_data.jsonl, test_data.jsonl, config.json\n",
        "3. –ò–Ω—Ç–µ—Ä–Ω–µ—Ç –≤–∫–ª—é—á–µ–Ω –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
        "!pip install -q transformers==4.36.0\n",
        "!pip install -q peft==0.7.1\n",
        "!pip install -q trl==0.7.4\n",
        "!pip install -q bitsandbytes==0.41.3\n",
        "!pip install -q accelerate==0.25.0\n",
        "!pip install -q datasets==2.15.0\n",
        "\n",
        "print(\"‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ò–º–ø–æ—Ä—Ç—ã\n",
        "import json\n",
        "import torch\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n",
        "from datasets import Dataset\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "print(f\"üî• CUDA –¥–æ—Å—Ç—É–ø–µ–Ω: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(\n",
        "        f\"üíæ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏\n",
        "# –ó–∞–º–µ–Ω–∏—Ç–µ '/kaggle/input/your-dataset' –Ω–∞ –ø—É—Ç—å –∫ –≤–∞—à–µ–º—É dataset –≤ Kaggle\n",
        "DATASET_PATH = \"/kaggle/input/hashtag-dataset\"  # –ò–∑–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à dataset\n",
        "\n",
        "config_file = Path(DATASET_PATH) / \"config.json\"\n",
        "with open(config_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "print(\"üìã –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞:\")\n",
        "print(f\"   –ú–æ–¥–µ–ª—å: {config['model_config']['base_model']}\")\n",
        "print(f\"   –û–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {config['dataset_info']['train_samples']}\")\n",
        "print(f\"   –¢–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {config['dataset_info']['test_samples']}\")\n",
        "print(f\"   LoRA rank: {config['training_config']['lora_r']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "def load_jsonl(file_path):\n",
        "    \"\"\"–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ JSONL —Ñ–∞–π–ª–∞\"\"\"\n",
        "    data = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—É—á–∞—é—â–∏–µ –∏ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
        "train_file = Path(DATASET_PATH) / \"train_data.jsonl\"\n",
        "test_file = Path(DATASET_PATH) / \"test_data.jsonl\"\n",
        "\n",
        "train_data = load_jsonl(train_file)\n",
        "test_data = load_jsonl(test_file)\n",
        "\n",
        "print(f\"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ:\")\n",
        "print(f\"   –û–±—É—á–∞—é—â–∏—Ö: {len(train_data)}\")\n",
        "print(f\"   –¢–µ—Å—Ç–æ–≤—ã—Ö: {len(test_data)}\")\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö\n",
        "print(f\"\\nüìù –ü—Ä–∏–º–µ—Ä –æ–±—É—á–∞—é—â–µ–≥–æ –ø—Ä–∏–º–µ—Ä–∞:\")\n",
        "print(f\"–í—Ö–æ–¥: {train_data[0]['input'][:100]}...\")\n",
        "print(f\"–í—ã—Ö–æ–¥: {train_data[0]['output']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–æ–∫ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (—Å–æ–∑–¥–∞—é—Ç—Å—è –≤ Kaggle)\n",
        "output_dir = \"./hashtag_lora_model\"\n",
        "merged_dir = \"./merged_hashtag_model\"\n",
        "final_dir = \"./final_model_for_ollama\"\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "os.makedirs(merged_dir, exist_ok=True)\n",
        "os.makedirs(final_dir, exist_ok=True)\n",
        "\n",
        "print(f\"üìÅ –ü–∞–ø–∫–∏ —Å–æ–∑–¥–∞–Ω—ã –≤ Kaggle:\")\n",
        "print(f\"   LoRA –º–æ–¥–µ–ª—å: {output_dir}\")\n",
        "print(f\"   –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å: {merged_dir}\")\n",
        "print(f\"   –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å: {final_dir}\")"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üîÑ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
        "\n",
        "–°–ª–µ–¥—É—é—â–∏–µ —è—á–µ–π–∫–∏ –≤—ã–ø–æ–ª–Ω—è—é—Ç:\n",
        "1. –ó–∞–≥—Ä—É–∑–∫—É –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫—É –º–æ–¥–µ–ª–∏\n",
        "2. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é LoRA\n",
        "3. –û–±—É—á–µ–Ω–∏–µ\n",
        "4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "5. –°–æ–∑–¥–∞–Ω–∏–µ –∞—Ä—Ö–∏–≤–∞ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è\n",
        "\n",
        "**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 1-3 —á–∞—Å–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ü–æ–ª–Ω—ã–π –∫–æ–¥ –æ–±—É—á–µ–Ω–∏—è (–æ–±—ä–µ–¥–∏–Ω–µ–Ω –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è)\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏\n",
        "model_name = config[\"model_config\"][\"base_model\"]\n",
        "print(f\"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_name}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=config[\"training_config\"][\"lora_r\"],\n",
        "    lora_alpha=config[\"training_config\"][\"lora_alpha\"],\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "print(\"‚úÖ –ú–æ–¥–µ–ª—å –∏ LoRA –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –∑–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è\n",
        "def format_dataset(examples):\n",
        "    \"\"\"–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\"\"\"\n",
        "    return [example[\"text\"] for example in examples]\n",
        "\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã\n",
        "train_texts = format_dataset(train_data)\n",
        "test_texts = format_dataset(test_data)\n",
        "\n",
        "train_dataset = Dataset.from_dict({\"text\": train_texts})\n",
        "test_dataset = Dataset.from_dict({\"text\": test_texts})\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=config[\"training_config\"][\"epochs\"],\n",
        "    per_device_train_batch_size=config[\"training_config\"][\"batch_size\"],\n",
        "    per_device_eval_batch_size=config[\"training_config\"][\"batch_size\"],\n",
        "    gradient_accumulation_steps=2,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    save_steps=100,\n",
        "    logging_steps=10,\n",
        "    learning_rate=config[\"training_config\"][\"learning_rate\"],\n",
        "    weight_decay=0.001,\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    report_to=None,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        ")\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    peft_config=lora_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    max_seq_length=512,\n",
        "    packing=False,\n",
        ")\n",
        "\n",
        "print(\"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...\")\n",
        "trainer.train()\n",
        "print(\"üéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
        "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ LoRA\n",
        "trainer.model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ LoRA —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é\n",
        "print(\"üîÑ –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ LoRA —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é...\")\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –±–µ–∑ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, torch_dtype=torch.float16, device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º LoRA –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º\n",
        "from peft import PeftModel\n",
        "\n",
        "model_with_lora = PeftModel.from_pretrained(base_model, output_dir)\n",
        "merged_model = model_with_lora.merge_and_unload()\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å\n",
        "merged_model.save_pretrained(merged_dir)\n",
        "tokenizer.save_pretrained(merged_dir)\n",
        "\n",
        "print(f\"‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {merged_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è Ollama\n",
        "print(\"üì¶ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è Ollama...\")\n",
        "\n",
        "# –ö–æ–ø–∏—Ä—É–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –≤ —Ñ–∏–Ω–∞–ª—å–Ω—É—é –ø–∞–ø–∫—É\n",
        "shutil.copytree(merged_dir, final_dir, dirs_exist_ok=True)\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º Modelfile –¥–ª—è Ollama\n",
        "modelfile_content = '''FROM ./\n",
        "\n",
        "SYSTEM \"\"\"–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ö–µ—à—Ç–µ–≥–æ–≤ –¥–ª—è –Ω–æ–≤–æ—Å—Ç–µ–π, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–π –Ω–∞ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.\n",
        "\n",
        "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ö–µ—à—Ç–µ–≥–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.\n",
        "\n",
        "–ü—Ä–∞–≤–∏–ª–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:\n",
        "1. –°–æ–∑–¥–∞–≤–∞–π 3-5 —Ö–µ—à—Ç–µ–≥–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π –Ω–æ–≤–æ—Å—Ç–∏\n",
        "2. –•–µ—à—Ç–µ–≥–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –∏ –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω—ã–º–∏\n",
        "3. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫\n",
        "4. –ù–µ –¥–æ–±–∞–≤–ª—è–π —Å–∏–º–≤–æ–ª # –≤ –æ—Ç–≤–µ—Ç–µ\n",
        "5. –†–∞–∑–¥–µ–ª—è–π —Ö–µ—à—Ç–µ–≥–∏ –∑–∞–ø—è—Ç—ã–º–∏\n",
        "6. –§–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ–º–∞—Ö –∏ —Å–æ–±—ã—Ç–∏—è—Ö\n",
        "\n",
        "–û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ —Ö–µ—à—Ç–µ–≥–∞–º–∏, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤.\"\"\"\n",
        "\n",
        "PARAMETER temperature 0.1\n",
        "PARAMETER top_p 0.9\n",
        "PARAMETER repeat_penalty 1.1\n",
        "PARAMETER num_ctx 2048\n",
        "PARAMETER stop \"<s>\"\n",
        "PARAMETER stop \"</s>\"\n",
        "'''\n",
        "\n",
        "with open(f\"{final_dir}/Modelfile\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(modelfile_content)\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é\n",
        "instructions = f\"\"\"# üöÄ –î–æ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å Saiga LLaMA3 –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ö–µ—à—Ç–µ–≥–æ–≤\n",
        "\n",
        "## üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ–±—É—á–µ–Ω–∏–∏\n",
        "- –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: {model_name}\n",
        "- –û–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {config['dataset_info']['train_samples']}\n",
        "- –≠–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è: {config['training_config']['epochs']}\n",
        "- LoRA rank: {config['training_config']['lora_r']}\n",
        "\n",
        "## üîß –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤ Ollama\n",
        "\n",
        "1. –°–∫–∞—á–∞–π—Ç–µ –∏ —Ä–∞—Å–ø–∞–∫—É–π—Ç–µ —ç—Ç—É –ø–∞–ø–∫—É\n",
        "2. –ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤ –ø–∞–ø–∫—É —Å –º–æ–¥–µ–ª—å—é:\n",
        "   ```bash\n",
        "   cd path/to/final_model_for_ollama\n",
        "   ```\n",
        "\n",
        "3. –°–æ–∑–¥–∞–π—Ç–µ –º–æ–¥–µ–ª—å –≤ Ollama:\n",
        "   ```bash\n",
        "   ollama create saiga-hashtag-pro -f Modelfile\n",
        "   ```\n",
        "\n",
        "4. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∫—É:\n",
        "   ```bash\n",
        "   ollama list\n",
        "   ```\n",
        "\n",
        "## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
        "\n",
        "```bash\n",
        "ollama run saiga-hashtag-pro \"–¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –±–∞–Ω–∫ –ø–æ–≤—ã—Å–∏–ª –∫–ª—é—á–µ–≤—É—é —Å—Ç–∞–≤–∫—É\"\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "with open(f\"{final_dir}/README.md\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(instructions)\n",
        "\n",
        "print(f\"‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–∞ –≤ {final_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –°–æ–∑–¥–∞–Ω–∏–µ –∞—Ä—Ö–∏–≤–∞ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è\n",
        "print(\"üì¶ –°–æ–∑–¥–∞–Ω–∏–µ –∞—Ä—Ö–∏–≤–∞ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è...\")\n",
        "\n",
        "shutil.make_archive(\"saiga_hashtag_model_for_ollama\", \"zip\", final_dir)\n",
        "\n",
        "# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–∞–∑–º–µ—Ä–∞—Ö\n",
        "archive_size = os.path.getsize(\"saiga_hashtag_model_for_ollama.zip\") / (1024**3)\n",
        "print(f\"‚úÖ –ê—Ä—Ö–∏–≤ —Å–æ–∑–¥–∞–Ω: saiga_hashtag_model_for_ollama.zip\")\n",
        "print(f\"üìä –†–∞–∑–º–µ—Ä –∞—Ä—Ö–∏–≤–∞: {archive_size:.2f} GB\")\n",
        "\n",
        "print(\"\\nüéâ –ì–û–¢–û–í–û!\")\n",
        "print(\"üì• –°–∫–∞—á–∞–π—Ç–µ —Ñ–∞–π–ª 'saiga_hashtag_model_for_ollama.zip'\")\n",
        "print(\"üöÄ –°–ª–µ–¥—É–π—Ç–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –≤ README.md –≤–Ω—É—Ç—Ä–∏ –∞—Ä—Ö–∏–≤–∞\")\n",
        "print(\"\\nüìã –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:\")\n",
        "print(\"1. –°–∫–∞—á–∞–π—Ç–µ –∞—Ä—Ö–∏–≤ –∏–∑ Kaggle\")\n",
        "print(\"2. –†–∞—Å–ø–∞–∫—É–π—Ç–µ –Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–∞—à–∏–Ω–µ\")\n",
        "print(\"3. –í—ã–ø–æ–ª–Ω–∏—Ç–µ –∫–æ–º–∞–Ω–¥—É: ollama create saiga-hashtag-pro -f Modelfile\")\n",
        "print(\"4. –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–π—Ç–µ –≤ –≤–∞—à –ø—Ä–æ–µ–∫—Ç —Å–æ–≥–ª–∞—Å–Ω–æ KAGGLE_TO_OLLAMA_GUIDE.md\")"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üéØ –ò—Ç–æ–≥–∏ –æ–±—É—á–µ–Ω–∏—è\n",
        "\n",
        "## ‚úÖ –ß—Ç–æ —Å–¥–µ–ª–∞–Ω–æ:\n",
        "1. **–ó–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞** –º–æ–¥–µ–ª—å Saiga LLaMA3\n",
        "2. **–ü—Ä–∏–º–µ–Ω–µ–Ω LoRA** –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è\n",
        "3. **–û–±—É—á–µ–Ω–∞ –º–æ–¥–µ–ª—å** –Ω–∞ –≤–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "4. **–û–±—ä–µ–¥–∏–Ω–µ–Ω–∞ LoRA** —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é\n",
        "5. **–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω –∞—Ä—Ö–∏–≤** –¥–ª—è Ollama\n",
        "\n",
        "## üöÄ –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:\n",
        "1. **–°–∫–∞—á–∞–π—Ç–µ** `saiga_hashtag_model_for_ollama.zip`\n",
        "2. **–†–∞—Å–ø–∞–∫—É–π—Ç–µ** –∞—Ä—Ö–∏–≤ –Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–∞—à–∏–Ω–µ\n",
        "3. **–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –º–æ–¥–µ–ª—å** –≤ Ollama –∫–æ–º–∞–Ω–¥–æ–π `ollama create`\n",
        "4. **–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–π—Ç–µ** –≤ –≤–∞—à –ø—Ä–æ–µ–∫—Ç —Å–æ–≥–ª–∞—Å–Ω–æ –≥–∞–π–¥—É\n",
        "\n",
        "## üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:\n",
        "- **LoRA rank**: 16\n",
        "- **–≠–ø–æ—Ö–∏**: 3\n",
        "- **Batch size**: 4\n",
        "- **Learning rate**: 2e-4\n",
        "\n",
        "**–£—Å–ø–µ—à–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è! üéâ**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
