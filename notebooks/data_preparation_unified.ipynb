{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "**–≠—Ç–∞–ø—ã:**\n",
        "1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ PostgreSQL\n",
        "2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤ —Ñ–æ—Ä–º–∞—Ç ChatML –¥–ª—è Saiga LLaMA3\n",
        "3. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ö–µ—à—Ç–µ–≥–æ–≤ –ø–æ —á–∞—Å—Ç–æ—Ç–µ\n",
        "4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
        "\n",
        "**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –ì–æ—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤ Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ò–º–ø–æ—Ä—Ç—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏\n",
        "import asyncio\n",
        "import asyncpg\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from typing import List, Dict, Any"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è\n",
        "DB_CONFIG = {\n",
        "    \"host\": \"localhost\",\n",
        "    \"port\": 5432,\n",
        "    \"database\": \"news_analyzer\",\n",
        "    \"user\": \"\",  # –ò–∑–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à PostgreSQL –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å\n",
        "    \"password\": \"\",  # –ò–∑–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à PostgreSQL –ø–∞—Ä–æ–ª—å\n",
        "}\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —ç–∫—Å–ø–æ—Ä—Ç–∞\n",
        "OUTPUT_DIR = \"./kaggle_dataset\"\n",
        "MAX_SAMPLES = 10000\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏\n",
        "MIN_FREQ_TRAIN = 14  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ —Ö–µ—à—Ç–µ–≥–∞ –¥–ª—è train\n",
        "MIN_FREQ_TEST = 6  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ —Ö–µ—à—Ç–µ–≥–∞ –¥–ª—è test\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"üìÅ –ü–∞–ø–∫–∞ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞: {OUTPUT_DIR}\")\n",
        "print(\n",
        "    f\"üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: train_freq={MIN_FREQ_TRAIN}, test_freq={MIN_FREQ_TEST}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö\n",
        "async def load_training_data():\n",
        "    \"\"\"–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏–∑ PostgreSQL\"\"\"\n",
        "    try:\n",
        "        print(\"üîå –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL...\")\n",
        "        conn = await asyncpg.connect(**DB_CONFIG)\n",
        "\n",
        "        query = \"\"\"\n",
        "        SELECT \n",
        "            m.text,\n",
        "            a.hashtags,\n",
        "            a.sentiment,\n",
        "            m.channel_title\n",
        "        FROM messages m\n",
        "        JOIN analyses a ON m.message_id = a.message_id\n",
        "        WHERE \n",
        "            LENGTH(m.text) BETWEEN 50 AND 2000\n",
        "            AND a.hashtags IS NOT NULL\n",
        "            AND a.hashtags != '[]'\n",
        "            AND a.hashtags != 'null'\n",
        "            AND jsonb_array_length(a.hashtags::jsonb) BETWEEN 2 AND 10\n",
        "        ORDER BY m.date DESC\n",
        "        LIMIT $1\n",
        "        \"\"\"\n",
        "\n",
        "        print(f\"üìä –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ (–º–∞–∫—Å. {MAX_SAMPLES})...\")\n",
        "        rows = await conn.fetch(query, MAX_SAMPLES)\n",
        "        await conn.close()\n",
        "\n",
        "        print(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(rows)} –∑–∞–ø–∏—Å–µ–π\")\n",
        "        return rows\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}\")\n",
        "        print(\"üí° –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ PostgreSQL –∑–∞–ø—É—â–µ–Ω –∏ –¥–æ—Å—Ç—É–ø–µ–Ω\")\n",
        "        return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö\n",
        "def prepare_training_examples(raw_data):\n",
        "    \"\"\"–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–∏–º–µ—Ä—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ ChatML –¥–ª—è Saiga LLaMA3\"\"\"\n",
        "    training_examples = []\n",
        "    system_prompt = \"–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ö–µ—à—Ç–µ–≥–æ–≤ –¥–ª—è –Ω–æ–≤–æ—Å—Ç–µ–π. –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π 3-5 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ö–µ—à—Ç–µ–≥–æ–≤ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.\"\n",
        "\n",
        "    for row in raw_data:\n",
        "        try:\n",
        "            hashtags = json.loads(row[\"hashtags\"])\n",
        "            hashtags_str = \", \".join(hashtags)\n",
        "\n",
        "            # –§–æ—Ä–º–∞—Ç ChatML –¥–ª—è Saiga LLaMA3\n",
        "            full_prompt = f\"<s>system\\\\n{system_prompt}</s>\\\\n<s>user\\\\n{row['text']}</s>\\\\n<s>assistant\\\\n{hashtags_str}</s>\"\n",
        "\n",
        "            training_examples.append(\n",
        "                {\n",
        "                    \"text\": full_prompt,\n",
        "                    \"input\": row[\"text\"],\n",
        "                    \"output\": hashtags_str,\n",
        "                    \"channel\": row[\"channel_title\"],\n",
        "                    \"sentiment\": row[\"sentiment\"],\n",
        "                }\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø–∏—Å–∏: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(training_examples)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "    return training_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö\n",
        "def filter_hashtags_by_frequency(data: List[Dict], min_freq: int = 14) -> List[Dict]:\n",
        "    \"\"\"–§–∏–ª—å—Ç—Ä—É–µ—Ç —Ö–µ—à—Ç–µ–≥–∏ –ø–æ —á–∞—Å—Ç–æ—Ç–µ –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏\"\"\"\n",
        "\n",
        "    def flatten(items):\n",
        "        \"\"\"–†–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ—Ç –≤–ª–æ–∂–µ–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏\"\"\"\n",
        "        for x in items:\n",
        "            if isinstance(x, list):\n",
        "                yield from flatten(x)\n",
        "            else:\n",
        "                yield x\n",
        "\n",
        "    # 1. –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—ã —Ö–µ—à—Ç–µ–≥–æ–≤\n",
        "    counter = Counter()\n",
        "    for item in data:\n",
        "        output = item.get(\"output\", \"\")\n",
        "        if output:\n",
        "            hashtags = [tag.strip() for tag in output.split(\",\") if tag.strip()]\n",
        "            counter.update(hashtags)\n",
        "\n",
        "    # 2. –û–ø—Ä–µ–¥–µ–ª—è–µ–º —á–∞—Å—Ç—ã–µ —Ö–µ—à—Ç–µ–≥–∏\n",
        "    frequent_tags = {tag for tag, freq in counter.items() if freq >= min_freq}\n",
        "    print(f\"üìä –ù–∞–π–¥–µ–Ω–æ {len(frequent_tags)} —Ö–µ—à—Ç–µ–≥–æ–≤ —Å —á–∞—Å—Ç–æ—Ç–æ–π >= {min_freq}\")\n",
        "\n",
        "    # 3. –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ\n",
        "    filtered_data = []\n",
        "    for item in data:\n",
        "        output = item.get(\"output\", \"\")\n",
        "        if output:\n",
        "            hashtags = [tag.strip() for tag in output.split(\",\") if tag.strip()]\n",
        "            filtered_hashtags = [tag for tag in hashtags if tag in frequent_tags]\n",
        "\n",
        "            if filtered_hashtags:  # –ï—Å–ª–∏ –µ—Å—Ç—å —Ö–µ—à—Ç–µ–≥–∏ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏\n",
        "                new_output = \", \".join(filtered_hashtags)\n",
        "\n",
        "                # –û–±–Ω–æ–≤–ª—è–µ–º output\n",
        "                filtered_item = item.copy()\n",
        "                filtered_item[\"output\"] = new_output\n",
        "\n",
        "                # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º assistant-—á–∞—Å—Ç—å –≤ text\n",
        "                if \"text\" in filtered_item:\n",
        "                    parts = filtered_item[\"text\"].rsplit(\"<s>assistant\\\\n\", 1)\n",
        "                    if len(parts) == 2:\n",
        "                        filtered_item[\"text\"] = (\n",
        "                            parts[0] + \"<s>assistant\\\\n\" + new_output + \"</s>\"\n",
        "                        )\n",
        "\n",
        "                filtered_data.append(filtered_item)\n",
        "\n",
        "    print(f\"‚úÖ –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(filtered_data)} –∑–∞–ø–∏—Å–µ–π\")\n",
        "    return filtered_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö\n",
        "def save_final_dataset(train_data, test_data, output_dir=OUTPUT_DIR):\n",
        "    \"\"\"–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç\"\"\"\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ JSONL —Ñ–æ—Ä–º–∞—Ç–µ\n",
        "    train_file = Path(output_dir) / \"train_data_final.jsonl\"\n",
        "    test_file = Path(output_dir) / \"test_data_final.jsonl\"\n",
        "\n",
        "    with open(train_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for item in train_data:\n",
        "            f.write(json.dumps(item, ensure_ascii=False) + \"\\\\n\")\n",
        "\n",
        "    with open(test_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for item in test_data:\n",
        "            f.write(json.dumps(item, ensure_ascii=False) + \"\\\\n\")\n",
        "\n",
        "    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è Kaggle\n",
        "    config = {\n",
        "        \"dataset_info\": {\n",
        "            \"total_samples\": len(train_data) + len(test_data),\n",
        "            \"train_samples\": len(train_data),\n",
        "            \"test_samples\": len(test_data),\n",
        "            \"created_at\": datetime.now().isoformat(),\n",
        "            \"filtered\": True,\n",
        "            \"min_freq_train\": MIN_FREQ_TRAIN,\n",
        "            \"min_freq_test\": MIN_FREQ_TEST,\n",
        "        },\n",
        "        \"model_config\": {\n",
        "            \"base_model\": \"IlyaGusev/saiga_llama3_8b\",\n",
        "            \"task\": \"hashtag_generation\",\n",
        "            \"format\": \"ChatML\",\n",
        "        },\n",
        "        \"training_config\": {\n",
        "            \"epochs\": 3,\n",
        "            \"batch_size\": 4,\n",
        "            \"learning_rate\": 2e-4,\n",
        "            \"lora_r\": 16,\n",
        "            \"lora_alpha\": 32,\n",
        "        },\n",
        "    }\n",
        "\n",
        "    config_file = Path(output_dir) / \"config.json\"\n",
        "    with open(config_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # README\n",
        "    readme_content = f\"\"\"# –§–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è Saiga LLaMA3\n",
        "\n",
        "- –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(train_data) + len(test_data)}\n",
        "- –û–±—É—á–∞—é—â–∏—Ö: {len(train_data)}\n",
        "- –¢–µ—Å—Ç–æ–≤—ã—Ö: {len(test_data)}\n",
        "- –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: –î–∞ (min_freq_train={MIN_FREQ_TRAIN}, min_freq_test={MIN_FREQ_TEST})\n",
        "\n",
        "- `train_data_final.jsonl` - –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ (–æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ)\n",
        "- `test_data_final.jsonl` - —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ (–æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ)\n",
        "- `config.json` - –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è\n",
        "\n",
        "–î–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤ Kaggle –∏ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.\n",
        "\"\"\"\n",
        "\n",
        "    readme_file = Path(output_dir) / \"README.md\"\n",
        "    with open(readme_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(readme_content)\n",
        "\n",
        "    print(f\"‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_dir}/\")\n",
        "    print(f\"üìÑ –§–∞–π–ª—ã:\")\n",
        "    print(f\"   - train_data_final.jsonl ({len(train_data)} –∑–∞–ø–∏—Å–µ–π)\")\n",
        "    print(f\"   - test_data_final.jsonl ({len(test_data)} –∑–∞–ø–∏—Å–µ–π)\")\n",
        "    print(f\"   - config.json\")\n",
        "    print(f\"   - README.md\")\n",
        "\n",
        "    return {\n",
        "        \"train_samples\": len(train_data),\n",
        "        \"test_samples\": len(test_data),\n",
        "        \"total_samples\": len(train_data) + len(test_data),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –û—Å–Ω–æ–≤–Ω–æ–π pipeline - –≠—Ç–∞–ø 1: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "print(\"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –µ–¥–∏–Ω—ã–π pipeline –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ PostgreSQL\n",
        "raw_data = await load_training_data()\n",
        "\n",
        "if not raw_data:\n",
        "    print(\"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ PostgreSQL\")\n",
        "    print(\"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏\")\n",
        "else:\n",
        "    print(f\"–≠—Ç–∞–ø 1 –∑–∞–≤–µ—Ä—à–µ–Ω: –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(raw_data)} –∑–∞–ø–∏—Å–µ–π\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –≠—Ç–∞–ø 2: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤ —Ñ–æ—Ä–º–∞—Ç ChatML\n",
        "if raw_data:\n",
        "    print(\"\\\\n–≠—Ç–∞–ø 2: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–æ—Ä–º–∞—Ç ChatML\")\n",
        "    training_examples = prepare_training_examples(raw_data)\n",
        "\n",
        "    if training_examples:\n",
        "        print(f\"–≠—Ç–∞–ø 2 –∑–∞–≤–µ—Ä—à–µ–Ω: –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(training_examples)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "\n",
        "        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä\n",
        "        print(\"\\\\n–ü—Ä–∏–º–µ—Ä –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "        print(f\"Input: {training_examples[0]['input'][:100]}...\")\n",
        "        print(f\"Output: {training_examples[0]['output']}\")\n",
        "        print(f\"Text format: {training_examples[0]['text'][:150]}...\")\n",
        "    else:\n",
        "        print(\"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã\")\n",
        "        training_examples = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –≠—Ç–∞–ø 3: –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è\n",
        "if training_examples:\n",
        "    print(\"\\\\n–≠—Ç–∞–ø 3: –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è\")\n",
        "\n",
        "    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/test (80/20)\n",
        "    split_index = int(len(training_examples) * 0.8)\n",
        "    train_data_raw = training_examples[:split_index]\n",
        "    test_data_raw = training_examples[split_index:]\n",
        "\n",
        "    print(f\"–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ: {len(train_data_raw)} train, {len(test_data_raw)} test\")\n",
        "\n",
        "    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ö–µ—à—Ç–µ–≥–∏ –ø–æ —á–∞—Å—Ç–æ—Ç–µ\n",
        "    print(\"\\\\n–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ö–µ—à—Ç–µ–≥–æ–≤ –ø–æ —á–∞—Å—Ç–æ—Ç–µ...\")\n",
        "    print(f\"Train –¥–∞–Ω–Ω—ã–µ (min_freq={MIN_FREQ_TRAIN}):\")\n",
        "    train_data_filtered = filter_hashtags_by_frequency(train_data_raw, MIN_FREQ_TRAIN)\n",
        "\n",
        "    print(f\"\\\\nTest –¥–∞–Ω–Ω—ã–µ (min_freq={MIN_FREQ_TEST}):\")\n",
        "    test_data_filtered = filter_hashtags_by_frequency(test_data_raw, MIN_FREQ_TEST)\n",
        "\n",
        "    print(f\"\\\\n–≠—Ç–∞–ø 3 –∑–∞–≤–µ—Ä—à–µ–Ω:\")\n",
        "    print(\n",
        "        f\"   - Train: {len(train_data_filtered)} –∑–∞–ø–∏—Å–µ–π (–±—ã–ª–æ {len(train_data_raw)})\"\n",
        "    )\n",
        "    print(f\"   - Test: {len(test_data_filtered)} –∑–∞–ø–∏—Å–µ–π (–±—ã–ª–æ {len(test_data_raw)})\")\n",
        "\n",
        "    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏\n",
        "    train_reduction = (\n",
        "        (len(train_data_raw) - len(train_data_filtered)) / len(train_data_raw) * 100\n",
        "    )\n",
        "    test_reduction = (\n",
        "        (len(test_data_raw) - len(test_data_filtered)) / len(test_data_raw) * 100\n",
        "    )\n",
        "    print(\n",
        "        f\"   - –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —É–¥–∞–ª–∏–ª–∞: {train_reduction:.1f}% train, {test_reduction:.1f}% test\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –≠—Ç–∞–ø 4: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
        "if \"train_data_filtered\" in locals() and \"test_data_filtered\" in locals():\n",
        "    print(\"\\\\n–≠—Ç–∞–ø 4: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\")\n",
        "\n",
        "    result = save_final_dataset(train_data_filtered, test_data_filtered)\n",
        "\n",
        "    print(f\"\\\\nPipeline –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"–ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\")\n",
        "    print(f\"   - –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {result['total_samples']}\")\n",
        "    print(f\"   - –û–±—É—á–∞—é—â–∏—Ö: {result['train_samples']}\")\n",
        "    print(f\"   - –¢–µ—Å—Ç–æ–≤—ã—Ö: {result['test_samples']}\")\n",
        "    print(f\"   - –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∞: –î–∞\")\n",
        "    print(f\"   - –§–æ—Ä–º–∞—Ç: ChatML –¥–ª—è Saiga LLaMA3\")\n",
        "    print(\"\\\\n –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:\")\n",
        "    print(\"   1. –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª—ã –∏–∑ kaggle_dataset/ –≤ Kaggle\")\n",
        "    print(\"   2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\")\n",
        "    print(\"   3. –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–π—Ç–µ –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –≤ Ollama\")\n",
        "\n",
        "else:\n",
        "    print(\"Pipeline –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞–≤–µ—Ä—à–µ–Ω –∏–∑-–∑–∞ –æ—à–∏–±–æ–∫ –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —ç—Ç–∞–ø–∞—Ö\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
