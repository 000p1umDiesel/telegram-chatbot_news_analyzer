#!/usr/bin/env python3
"""
–¢–µ—Å—Ç –∫–∞—á–µ—Å—Ç–≤–∞ —Ö–µ—à—Ç–µ–≥–æ–≤: —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ.
"""

from typing import List, Dict, Any, Set
import numpy as np
from collections import Counter
from sentence_transformers import SentenceTransformer
from .base_evaluator import BaseEvaluator


class HashtagTest(BaseEvaluator):
    """–¢–µ—Å—Ç –∫–∞—á–µ—Å—Ç–≤–∞ —Ö–µ—à—Ç–µ–≥–æ–≤."""

    def __init__(self, db_config: dict = None):
        super().__init__(db_config)
        self._load_models()

    def _load_models(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ö–µ—à—Ç–µ–≥–æ–≤."""
        try:
            print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ö–µ—à—Ç–µ–≥–æ–≤...")
            self.sentence_model = SentenceTransformer(
                "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
            )
            print("‚úÖ –ú–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ö–µ—à—Ç–µ–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            self.sentence_model = None

    def run_test(self, data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –∫–∞—á–µ—Å—Ç–≤–∞ —Ö–µ—à—Ç–µ–≥–æ–≤."""
        print("üîç –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ö–µ—à—Ç–µ–≥–æ–≤...")

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∏ —Ö–µ—à—Ç–µ–≥–∏
        texts = [item["text"] for item in data]
        hashtags_lists = [item["hashtags"] for item in data]

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —Ö–µ—à—Ç–µ–≥–æ–≤
        hashtag_metrics = self._analyze_hashtag_quality(texts, hashtags_lists)

        self.results["metrics"] = hashtag_metrics
        self.results["details"] = {
            "samples_processed": len(data),
            "total_unique_hashtags": len(self._get_all_hashtags(hashtags_lists)),
            "average_hashtags_per_text": np.mean(
                [len(tags) for tags in hashtags_lists if tags]
            ),
        }

        return self.results

    def _analyze_hashtag_quality(
        self, texts: List[str], hashtags_lists: List[List[str]]
    ) -> Dict[str, float]:
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —Ö–µ—à—Ç–µ–≥–æ–≤."""
        relevance_scores = []
        diversity_scores = []
        coverage_scores = []

        for text, hashtags in zip(texts, hashtags_lists):
            if not text or not hashtags:
                continue

            # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ö–µ—à—Ç–µ–≥–æ–≤ –∫ —Ç–µ–∫—Å—Ç—É
            relevance = self._compute_hashtag_relevance(text, hashtags)
            relevance_scores.append(relevance)

            # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ö–µ—à—Ç–µ–≥–æ–≤
            diversity = self._compute_hashtag_diversity(hashtags)
            diversity_scores.append(diversity)

            # –ü–æ–∫—Ä—ã—Ç–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ–º
            coverage = self._compute_topic_coverage(text, hashtags)
            coverage_scores.append(coverage)

        return {
            "hashtag_relevance_mean": (
                np.mean(relevance_scores) if relevance_scores else 0.0
            ),
            "hashtag_relevance_std": (
                np.std(relevance_scores) if relevance_scores else 0.0
            ),
            "hashtag_diversity_mean": (
                np.mean(diversity_scores) if diversity_scores else 0.0
            ),
            "hashtag_diversity_std": (
                np.std(diversity_scores) if diversity_scores else 0.0
            ),
            "topic_coverage_mean": np.mean(coverage_scores) if coverage_scores else 0.0,
            "topic_coverage_std": np.std(coverage_scores) if coverage_scores else 0.0,
            "samples_with_hashtags": len(relevance_scores),
        }

    def _compute_hashtag_relevance(self, text: str, hashtags: List[str]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Ö–µ—à—Ç–µ–≥–æ–≤ –∫ —Ç–µ–∫—Å—Ç—É."""
        if not self.sentence_model or not hashtags:
            return 0.0

        try:
            # –ü–æ–ª—É—á–∞–µ–º embedding —Ç–µ–∫—Å—Ç–∞
            text_embedding = self.sentence_model.encode([text[:500]])[0]

            # –ü–æ–ª—É—á–∞–µ–º embeddings —Ö–µ—à—Ç–µ–≥–æ–≤
            hashtag_texts = [f"#{tag}" for tag in hashtags]
            hashtag_embeddings = self.sentence_model.encode(hashtag_texts)

            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            similarities = []
            for hashtag_emb in hashtag_embeddings:
                similarity = np.dot(text_embedding, hashtag_emb) / (
                    np.linalg.norm(text_embedding) * np.linalg.norm(hashtag_emb)
                )
                similarities.append(float(similarity))

            return np.mean(similarities) if similarities else 0.0

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ _compute_hashtag_relevance: {e}")
            return 0.0

    def _compute_hashtag_diversity(self, hashtags: List[str]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —Ö–µ—à—Ç–µ–≥–æ–≤."""
        if not hashtags:
            return 0.0

        # –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å —Ö–µ—à—Ç–µ–≥–æ–≤
        unique_ratio = len(set(hashtags)) / len(hashtags)

        # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –¥–ª–∏–Ω —Ö–µ—à—Ç–µ–≥–æ–≤
        lengths = [len(tag) for tag in hashtags]
        length_std = np.std(lengths) if len(lengths) > 1 else 0

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º std –¥–ª–∏–Ω—ã (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø–æ–ª–µ–∑–Ω–∞—è std ~ 5)
        normalized_length_diversity = min(length_std / 5.0, 1.0)

        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏
        diversity_score = (unique_ratio + normalized_length_diversity) / 2

        return float(diversity_score)

    def _compute_topic_coverage(self, text: str, hashtags: List[str]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ–∫—Ä—ã—Ç–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ–º –≤ —Ç–µ–∫—Å—Ç–µ."""
        if not hashtags:
            return 0.0

        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –≤ —Ö–µ—à—Ç–µ–≥–∞—Ö
        text_words = set(text.lower().split())
        hashtag_words = set()

        for tag in hashtags:
            hashtag_words.update(tag.lower().split())

        # –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
        common_words = text_words.intersection(hashtag_words)

        # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ–∫—Ä—ã—Ç–∏–µ
        if len(text_words) == 0:
            return 0.0

        coverage = len(common_words) / min(
            len(text_words), 20
        )  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 20 —Å–ª–æ–≤
        return min(coverage, 1.0)

    def _get_all_hashtags(self, hashtags_lists: List[List[str]]) -> Set[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ö–µ—à—Ç–µ–≥–æ–≤."""
        all_hashtags = set()
        for hashtags in hashtags_lists:
            if hashtags:
                all_hashtags.update(hashtags)
        return all_hashtags

    def _generate_summary(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫—Ä–∞—Ç–∫–æ–≥–æ —Ä–µ–∑—é–º–µ —Ç–µ—Å—Ç–∞ —Ö–µ—à—Ç–µ–≥–æ–≤."""
        metrics = self.results["metrics"]

        relevance = metrics.get("hashtag_relevance_mean", 0)
        diversity = metrics.get("hashtag_diversity_mean", 0)
        coverage = metrics.get("topic_coverage_mean", 0)

        return f"Relevance: {relevance:.3f}, Diversity: {diversity:.3f}, Coverage: {coverage:.3f}"

    def _generate_recommendations(self) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ —Ö–µ—à—Ç–µ–≥–æ–≤."""
        metrics = self.results["metrics"]
        recommendations = []

        relevance = metrics.get("hashtag_relevance_mean", 0)
        diversity = metrics.get("hashtag_diversity_mean", 0)
        coverage = metrics.get("topic_coverage_mean", 0)

        if relevance < 0.5:
            recommendations.append(
                "–ù–∏–∑–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ö–µ—à—Ç–µ–≥–æ–≤ - —É–ª—É—á—à–∏—Ç–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ö–µ—à—Ç–µ–≥–æ–≤ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é —Ç–µ–∫—Å—Ç–∞"
            )

        if diversity < 0.6:
            recommendations.append(
                "–ù–∏–∑–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ö–µ—à—Ç–µ–≥–æ–≤ - –¥–æ–±–∞–≤–ª—è–π—Ç–µ –±–æ–ª—å—à–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —Ö–µ—à—Ç–µ–≥–æ–≤"
            )

        if coverage < 0.3:
            recommendations.append(
                "–ù–∏–∑–∫–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ —Ç–µ–º - —Ö–µ—à—Ç–µ–≥–∏ –Ω–µ –ø–æ–∫—Ä—ã–≤–∞—é—Ç –∫–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"
            )

        if all(score > 0.7 for score in [relevance, diversity, coverage]):
            recommendations.append("–û—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Ö–µ—à—Ç–µ–≥–æ–≤!")

        return recommendations
