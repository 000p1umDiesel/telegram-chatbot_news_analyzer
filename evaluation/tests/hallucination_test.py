#!/usr/bin/env python3
"""
–¢–µ—Å—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏.
"""

from typing import List, Dict, Any
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
from .base_evaluator import BaseEvaluator


class HallucinationTest(BaseEvaluator):
    """–¢–µ—Å—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π."""

    def __init__(self, db_config: dict = None):
        super().__init__(db_config)
        self.hallucination_thresholds = [0.3, 0.5, 0.7]
        self._load_nli_model()

    def _load_nli_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ NLI –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π."""
        try:
            print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ NLI –º–æ–¥–µ–ª–∏ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π...")
            model_name = "cointegrated/rubert-base-cased-nli-threeway"

            self.nli_tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.nli_model = AutoModelForSequenceClassification.from_pretrained(
                model_name
            )

            # –°–æ–∑–¥–∞–µ–º pipeline –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
            self.nli_pipeline = pipeline(
                "text-classification",
                model=self.nli_model,
                tokenizer=self.nli_tokenizer,
                device=0 if torch.cuda.is_available() else -1,
            )

            print("‚úÖ NLI –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ NLI –º–æ–¥–µ–ª–∏: {e}")
            self.nli_model = None
            self.nli_pipeline = None

    def run_test(self, data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –¥–µ—Ç–µ–∫—Ü–∏–∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π."""
        print("üîç –ó–∞–ø—É—Å–∫ –¥–µ—Ç–µ–∫—Ü–∏–∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π...")

        if not self.nli_pipeline:
            print("‚ùå NLI –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
            return self.results

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∏ —Å–∞–º–º–∞—Ä–∏
        texts = [item["text"] for item in data]
        summaries = [item["summary"] for item in data]

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏
        hallucination_results = self._analyze_hallucinations(texts, summaries)

        self.results["metrics"] = hallucination_results
        self.results["details"] = {
            "samples_processed": len(data),
            "thresholds_used": self.hallucination_thresholds,
        }

        return self.results

    def _analyze_hallucinations(
        self, texts: List[str], summaries: List[str]
    ) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏."""
        entailment_scores = []
        hallucination_counts = {
            threshold: 0 for threshold in self.hallucination_thresholds
        }

        for text, summary in zip(texts, summaries):
            if not text or not summary:
                continue

            # –ü–æ–ª—É—á–∞–µ–º –æ—Ü–µ–Ω–∫—É entailment
            entailment_score = self._get_entailment_score(text, summary)
            entailment_scores.append(entailment_score)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ —Ä–∞–∑–Ω—ã–º –ø–æ—Ä–æ–≥–∞–º
            for threshold in self.hallucination_thresholds:
                if entailment_score < threshold:
                    hallucination_counts[threshold] += 1

        total_samples = len(entailment_scores)

        results = {
            "entailment_score_mean": (
                np.mean(entailment_scores) if entailment_scores else 0.0
            ),
            "entailment_score_std": (
                np.std(entailment_scores) if entailment_scores else 0.0
            ),
            "entailment_score_min": (
                np.min(entailment_scores) if entailment_scores else 0.0
            ),
            "entailment_score_max": (
                np.max(entailment_scores) if entailment_scores else 0.0
            ),
        }

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –ø–æ –ø–æ—Ä–æ–≥–∞–º
        for threshold in self.hallucination_thresholds:
            hallucination_rate = (
                hallucination_counts[threshold] / total_samples
                if total_samples > 0
                else 0
            )
            results[f"hallucination_rate_threshold_{threshold}"] = hallucination_rate

        return results

    def _get_entailment_score(self, premise: str, hypothesis: str) -> float:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ entailment –º–µ–∂–¥—É —Ç–µ–∫—Å—Ç–æ–º –∏ —Å—É–º–º–∞—Ä–∏."""
        try:
            # –û–±—Ä–µ–∑–∞–µ–º —Ç–µ–∫—Å—Ç—ã –¥–æ —Ä–∞–∑—É–º–Ω–æ–π –¥–ª–∏–Ω—ã
            premise = premise[:512]
            hypothesis = hypothesis[:256]

            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–ª—è NLI –º–æ–¥–µ–ª–∏
            input_text = f"{premise} [SEP] {hypothesis}"

            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            result = self.nli_pipeline(input_text)

            # –ò—â–µ–º –∫–ª–∞—Å—Å entailment
            entailment_score = 0.0
            for prediction in result:
                if "entailment" in prediction["label"].lower():
                    entailment_score = prediction["score"]
                    break

            return float(entailment_score)

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ _get_entailment_score: {e}")
            return 0.0

    def _generate_summary(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫—Ä–∞—Ç–∫–æ–≥–æ —Ä–µ–∑—é–º–µ —Ç–µ—Å—Ç–∞ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π."""
        metrics = self.results["metrics"]

        entailment_mean = metrics.get("entailment_score_mean", 0)
        hallucination_rate_05 = metrics.get("hallucination_rate_threshold_0.5", 0)

        return f"Entailment: {entailment_mean:.3f}, Hallucination Rate (0.5): {hallucination_rate_05:.3f}"

    def _generate_recommendations(self) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π."""
        metrics = self.results["metrics"]
        recommendations = []

        entailment_mean = metrics.get("entailment_score_mean", 0)
        hallucination_rate_05 = metrics.get("hallucination_rate_threshold_0.5", 0)

        if entailment_mean < 0.6:
            recommendations.append(
                "–ù–∏–∑–∫–∏–π entailment score - –º–æ–¥–µ–ª—å —á–∞—Å—Ç–æ —Å–æ–∑–¥–∞–µ—Ç —Å—É–º–º–∞—Ä–∏ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Ç–µ–∫—Å—Ç—É"
            )

        if hallucination_rate_05 > 0.3:
            recommendations.append(
                "–í—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π - –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É–ª—É—á—à–∏—Ç—å –∫–æ–Ω—Ç—Ä–æ–ª—å —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏"
            )

        if hallucination_rate_05 < 0.1:
            recommendations.append("–û—Ç–ª–∏—á–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π!")

        if entailment_mean > 0.8:
            recommendations.append(
                "–ü—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–∞—è –ª–æ–≥–∏—á–µ—Å–∫–∞—è —Å–≤—è–∑–Ω–æ—Å—Ç—å –º–µ–∂–¥—É —Ç–µ–∫—Å—Ç–æ–º –∏ —Å—É–º–º–∞—Ä–∏"
            )

        return recommendations
