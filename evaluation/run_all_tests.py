#!/usr/bin/env python3
"""
–ú–∞—Å—Ç–µ—Ä-—Å–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–µ–π.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ä–µ–∂–∏–º—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –æ—Ç—á–µ—Ç–æ–≤.
"""

import argparse
import subprocess
import sys
import json
import time
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor


class TestRunner:
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –∏ –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç –≤—Å–µ —Ç–µ—Å—Ç—ã —Å–∏—Å—Ç–µ–º—ã."""

    def __init__(self, db_path: Path = Path("data/storage.db")):
        self.db_path = db_path
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "test_suite_results": {},
            "execution_summary": {},
            "recommendations": [],
        }

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–µ—Å—Ç—ã
        self.available_tests = {
            "comprehensive": {
                "script": "evaluation/comprehensive_test.py",
                "description": "–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ (–≤–∫–ª—é—á–∞—è —É–ª—É—á—à–µ–Ω–Ω—É—é –¥–µ—Ç–µ–∫—Ü–∏—é –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π)",
                "default_args": ["--samples", "50"],
            },
            "rouge": {
                "script": "evaluation/rouge_metrics_test.py",
                "description": "ROUGE –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏",
                "default_args": ["--samples", "100"],
            },
            "performance": {
                "script": "evaluation/performance_test.py",
                "description": "–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏",
                "default_args": ["--samples", "20"],
            },
            "ab_prompt": {
                "script": "evaluation/ab_prompt_test.py",
                "description": "A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤",
                "default_args": ["--sample-size", "15"],
            },
        }

    def check_dependencies(self) -> Dict[str, bool]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π."""
        print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π...")

        dependencies = {
            "rouge-score": False,
            "nltk": False,
            "psutil": False,
            "numpy": False,
            "database": False,
        }

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º Python –ø–∞–∫–µ—Ç—ã
        try:
            import rouge_score

            dependencies["rouge-score"] = True
        except ImportError:
            pass

        try:
            import nltk

            dependencies["nltk"] = True
        except ImportError:
            pass

        try:
            import psutil

            dependencies["psutil"] = True
        except ImportError:
            pass

        try:
            import numpy

            dependencies["numpy"] = True
        except ImportError:
            pass

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        dependencies["database"] = self.db_path.exists()

        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for dep, available in dependencies.items():
            status = "‚úÖ" if available else "‚ùå"
            print(f"  {status} {dep}")

        return dependencies

    def install_missing_dependencies(self, dependencies: Dict[str, bool]):
        """–ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏."""
        missing = [
            dep
            for dep, available in dependencies.items()
            if not available and dep != "database"
        ]

        if missing:
            print(f"\n‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: {', '.join(missing)}")

            install_commands = {
                "rouge-score": "pip install rouge-score",
                "nltk": "pip install nltk",
                "psutil": "pip install psutil",
                "numpy": "pip install numpy",
            }

            print("üì¶ –ö–æ–º–∞–Ω–¥—ã –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏:")
            for dep in missing:
                if dep in install_commands:
                    print(f"  {install_commands[dep]}")

    def run_single_test(
        self, test_name: str, custom_args: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ç–µ—Å—Ç."""
        if test_name not in self.available_tests:
            return {
                "status": "failed",
                "error": f"Unknown test: {test_name}",
                "execution_time": 0,
            }

        test_config = self.available_tests[test_name]
        script_path = test_config["script"]

        if not Path(script_path).exists():
            return {
                "status": "failed",
                "error": f"Test script not found: {script_path}",
                "execution_time": 0,
            }

        print(f"\nüöÄ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞: {test_name}")
        print(f"üìù –û–ø–∏—Å–∞–Ω–∏–µ: {test_config['description']}")

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
        args = ["python", script_path]

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∏–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ
        if custom_args:
            args.extend(custom_args)
        else:
            args.extend(test_config["default_args"])

        # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
        args.extend(["--db", str(self.db_path)])

        # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –≤—ã–≤–æ–¥–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–∫—Ä–æ–º–µ comprehensive —Ç–µ—Å—Ç–∞)
        output_file = Path(f"evaluation/{test_name}_results.json")
        if test_name != "comprehensive":
            args.extend(["--output", str(output_file)])

        start_time = time.time()

        try:
            # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç
            result = subprocess.run(
                args,
                capture_output=True,
                text=True,
                timeout=600,  # 10 –º–∏–Ω—É—Ç –º–∞–∫—Å–∏–º—É–º –Ω–∞ —Ç–µ—Å—Ç
            )

            execution_time = time.time() - start_time

            if result.returncode == 0:
                # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                test_results = None
                if output_file.exists():
                    try:
                        with open(output_file, "r", encoding="utf-8") as f:
                            test_results = json.load(f)
                    except Exception as e:
                        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {e}")

                return {
                    "status": "passed",
                    "execution_time": execution_time,
                    "stdout": result.stdout,
                    "results": test_results,
                }
            else:
                return {
                    "status": "failed",
                    "execution_time": execution_time,
                    "error": result.stderr,
                    "stdout": result.stdout,
                }

        except subprocess.TimeoutExpired:
            return {
                "status": "timeout",
                "execution_time": time.time() - start_time,
                "error": "Test execution timeout (10 minutes)",
            }
        except Exception as e:
            return {
                "status": "error",
                "execution_time": time.time() - start_time,
                "error": str(e),
            }

    def run_tests_parallel(
        self, test_names: List[str], max_workers: int = 2
    ) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Ç–µ—Å—Ç—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ."""
        print(f"\nüîÑ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ (–º–∞–∫—Å. {max_workers} –ø–æ—Ç–æ–∫–æ–≤)")

        results = {}

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç—ã
            future_to_test = {
                executor.submit(self.run_single_test, test_name): test_name
                for test_name in test_names
            }

            # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for future in concurrent.futures.as_completed(future_to_test):
                test_name = future_to_test[future]
                try:
                    result = future.result()
                    results[test_name] = result

                    status_icon = "‚úÖ" if result["status"] == "passed" else "‚ùå"
                    print(
                        f"  {status_icon} {test_name}: {result['status']} ({result['execution_time']:.1f}s)"
                    )

                except Exception as e:
                    results[test_name] = {
                        "status": "error",
                        "error": str(e),
                        "execution_time": 0,
                    }
                    print(f"  ‚ùå {test_name}: error - {e}")

        return results

    def run_tests_sequential(self, test_names: List[str]) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Ç–µ—Å—Ç—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ."""
        print(f"\nüîÑ –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤")

        results = {}

        for test_name in test_names:
            result = self.run_single_test(test_name)
            results[test_name] = result

            status_icon = "‚úÖ" if result["status"] == "passed" else "‚ùå"
            print(
                f"  {status_icon} {test_name}: {result['status']} ({result['execution_time']:.1f}s)"
            )

        return results

    def analyze_results(self, test_results: Dict[str, Any]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤."""
        print(f"\nüìä –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")

        total_tests = len(test_results)
        passed_tests = sum(1 for r in test_results.values() if r["status"] == "passed")
        failed_tests = sum(1 for r in test_results.values() if r["status"] == "failed")
        error_tests = sum(
            1 for r in test_results.values() if r["status"] in ["error", "timeout"]
        )

        total_time = sum(r["execution_time"] for r in test_results.values())

        summary = {
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "failed_tests": failed_tests,
            "error_tests": error_tests,
            "success_rate": passed_tests / total_tests if total_tests > 0 else 0,
            "total_execution_time": total_time,
        }

        print(f"  üìà –í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: {total_tests}")
        print(f"  ‚úÖ –ü—Ä–æ–π–¥–µ–Ω–æ: {passed_tests}")
        print(f"  ‚ùå –ü—Ä–æ–≤–∞–ª–µ–Ω–æ: {failed_tests}")
        print(f"  üö´ –û—à–∏–±–∫–∏: {error_tests}")
        print(f"  üìä –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {summary['success_rate']:.1%}")
        print(f"  ‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.1f}—Å")

        return summary

    def generate_recommendations(self, test_results: Dict[str, Any]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–æ–≤."""
        recommendations = []

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–∞–∂–¥–æ–≥–æ —Ç–µ—Å—Ç–∞
        for test_name, result in test_results.items():
            if result["status"] != "passed":
                if result["status"] == "timeout":
                    recommendations.append(
                        f"‚è±Ô∏è –¢–µ—Å—Ç {test_name} –ø—Ä–µ–≤—ã—Å–∏–ª –ª–∏–º–∏—Ç –≤—Ä–µ–º–µ–Ω–∏. "
                        "–†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã."
                    )
                elif "results" not in result:
                    recommendations.append(
                        f"üîß –¢–µ—Å—Ç {test_name} –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π. "
                        "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏."
                    )
            else:
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —É—Å–ø–µ—à–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤
                if test_name == "comprehensive" and result.get("results"):
                    comp_results = result["results"]

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π
                    if "hallucination_detection" in comp_results:
                        halluc_rate = comp_results["hallucination_detection"].get(
                            "hallucination_rate", 0
                        )
                        if halluc_rate > 0.2:  # –ë–æ–ª–µ–µ 20% –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π
                            recommendations.append(
                                f"üß† –í—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π ({halluc_rate:.1%}). "
                                "–†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫—É —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏ –∏–ª–∏ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤."
                            )

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å sentiment analysis
                    if "sentiment_consistency" in comp_results:
                        consistency = comp_results["sentiment_consistency"].get(
                            "consistency_score", 0
                        )
                        if consistency < 0.8:  # –ú–µ–Ω–µ–µ 80% –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
                            recommendations.append(
                                f"üòê –ù–∏–∑–∫–∞—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ ({consistency:.1%}). "
                                "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–ª—É—á—à–∏—Ç—å –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏."
                            )

                elif test_name == "rouge" and result.get("results"):
                    rouge_results = result["results"]

                    if "rouge_evaluation" in rouge_results:
                        rouge_scores = rouge_results["rouge_evaluation"].get(
                            "aggregated_scores", {}
                        )
                        rouge1_f1 = rouge_scores.get("rouge1", {}).get("fmeasure", 0)

                        if rouge1_f1 < 0.3:  # –ù–∏–∑–∫–∏–µ ROUGE scores
                            recommendations.append(
                                f"üìù –ù–∏–∑–∫–∏–µ ROUGE-1 F1 scores ({rouge1_f1:.3f}). "
                                "–ö–∞—á–µ—Å—Ç–≤–æ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Ç—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è."
                            )

                elif test_name == "performance" and result.get("results"):
                    perf_results = result["results"]

                    if (
                        "tests" in perf_results
                        and "llm_performance" in perf_results["tests"]
                    ):
                        llm_perf = perf_results["tests"]["llm_performance"]
                        throughput = llm_perf.get("throughput_per_minute", 0)

                        if throughput < 5:  # –ú–µ–Ω–µ–µ 5 –∞–Ω–∞–ª–∏–∑–æ–≤ –≤ –º–∏–Ω—É—Ç—É
                            recommendations.append(
                                f"üêå –ù–∏–∑–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å LLM ({throughput:.1f} –∞–Ω–∞–ª–∏–∑–æ–≤/–º–∏–Ω). "
                                "–†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–æ–π –º–æ–¥–µ–ª–∏."
                            )

        # –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        failed_count = sum(1 for r in test_results.values() if r["status"] != "passed")
        if failed_count > 0:
            recommendations.append(
                f"üîß {failed_count} —Ç–µ—Å—Ç–æ–≤ –∑–∞–≤–µ—Ä—à–∏–ª–∏—Å—å –Ω–µ—É—Å–ø–µ—à–Ω–æ. "
                "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –∏ —É—Å—Ç—Ä–∞–Ω–∏—Ç–µ –ø—Ä–æ–±–ª–µ–º—ã –ø–µ—Ä–µ–¥ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–æ–º."
            )

        if not recommendations:
            recommendations.append(
                "üéâ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ—à–ª–∏ —É—Å–ø–µ—à–Ω–æ! –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é."
            )

        return recommendations

    def generate_report(self, test_results: Dict[str, Any], output_path: Path):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç –æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏."""
        summary = self.analyze_results(test_results)
        recommendations = self.generate_recommendations(test_results)

        self.results["test_suite_results"] = test_results
        self.results["execution_summary"] = summary
        self.results["recommendations"] = recommendations

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)

        print(f"\nüíæ –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")

        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        print(f"\nüìã –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
        for i, rec in enumerate(recommendations, 1):
            print(f"  {i}. {rec}")

    def run_test_suite(
        self, test_names: List[str], parallel: bool = False, max_workers: int = 2
    ) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤."""
        print(f"üöÄ –ó–∞–ø—É—Å–∫ –Ω–∞–±–æ—Ä–∞ —Ç–µ—Å—Ç–æ–≤: {', '.join(test_names)}")
        print(f"üìä –†–µ–∂–∏–º: {'–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π' if parallel else '–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π'}")
        print("=" * 60)

        start_time = time.time()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
        dependencies = self.check_dependencies()
        self.install_missing_dependencies(dependencies)

        # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç—ã
        if parallel and len(test_names) > 1:
            test_results = self.run_tests_parallel(test_names, max_workers)
        else:
            test_results = self.run_tests_sequential(test_names)

        total_time = time.time() - start_time

        print(f"\n‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {total_time:.1f}—Å")

        return test_results


def main():
    parser = argparse.ArgumentParser(
        description="–ú–∞—Å—Ç–µ—Ä-—Å–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–µ–π"
    )

    parser.add_argument(
        "--tests",
        nargs="+",
        choices=[
            "comprehensive",
            "rouge",
            "performance",
            "ab_prompt",
            "all",
        ],
        default=["all"],
        help="–¢–µ—Å—Ç—ã –¥–ª—è –∑–∞–ø—É—Å–∫–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: all)",
    )

    parser.add_argument(
        "--parallel", action="store_true", help="–ó–∞–ø—É—Å–∫–∞—Ç—å —Ç–µ—Å—Ç—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ"
    )

    parser.add_argument(
        "--max-workers",
        type=int,
        default=2,
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 2)",
    )

    parser.add_argument(
        "--db", type=Path, default=Path("data/storage.db"), help="–ü—É—Ç—å –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"
    )

    parser.add_argument(
        "--report",
        type=Path,
        default=Path("evaluation/test_suite_report.json"),
        help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞",
    )

    parser.add_argument(
        "--quick",
        action="store_true",
        help="–ë—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º —Å —É–º–µ–Ω—å—à–µ–Ω–Ω—ã–º–∏ —Ä–∞–∑–º–µ—Ä–∞–º–∏ –≤—ã–±–æ—Ä–æ–∫",
    )

    args = parser.parse_args()

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ø–∏—Å–æ–∫ —Ç–µ—Å—Ç–æ–≤
    runner = TestRunner(args.db)

    if "all" in args.tests:
        test_names = list(runner.available_tests.keys())
    else:
        test_names = args.tests

    # –í –±—ã—Å—Ç—Ä–æ–º —Ä–µ–∂–∏–º–µ —É–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã –≤—ã–±–æ—Ä–æ–∫
    if args.quick:
        print("‚ö° –ë—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º: —É–º–µ–Ω—å—à–µ–Ω–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –≤—ã–±–æ—Ä–æ–∫")
        # –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã –≤—ã–±–æ—Ä–æ–∫ –≤ 2 —Ä–∞–∑–∞
        for test_name, test_config in runner.available_tests.items():
            new_args = []
            i = 0
            while i < len(test_config["default_args"]):
                arg = test_config["default_args"][i]
                if arg in ["--samples", "--sample-size"] and i + 1 < len(
                    test_config["default_args"]
                ):
                    # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–ª–∞–≥ –∏ —É–º–µ–Ω—å—à–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                    new_args.append(arg)
                    original_value = int(test_config["default_args"][i + 1])
                    new_args.append(str(max(1, original_value // 2)))
                    i += 2  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–π –∞—Ä–≥—É–º–µ–Ω—Ç
                else:
                    new_args.append(arg)
                    i += 1
            test_config["default_args"] = new_args

    # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç—ã
    test_results = runner.run_test_suite(
        test_names, parallel=args.parallel, max_workers=args.max_workers
    )

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
    runner.generate_report(test_results, args.report)

    print(f"\nüéâ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")


if __name__ == "__main__":
    main()
